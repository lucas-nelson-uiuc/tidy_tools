{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"Functional PySpark Workflows <p>Tidy Tools is a declarative programming library promoting functional PySpark DataFrame workflows. The package is an extension of the PySpark API and can be easily integrated into existing code.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation: Set up <code>tidy_tools</code> in your environment.</li> <li>Getting Started: Learn how to build workflows step by step.</li> <li>API Reference: Explore all available functions and classes.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Functional: Packages verbose queries into iterative, declarative expressions.</li> <li>Feature-Rich: Extends the DataFrame API to include user-friendly features.</li> <li>Experimental: Continuously finding new ways of improving PySpark workflows.</li> </ul>"},{"location":"#philosophy","title":"Philosophy","text":"<p>The goal of Tidy Tools is to provide an extension of the PySpark DataFrame API that promotes declarative workflows.</p> <p>On top of the proposed API, Tidy Tools offers experimental solutions that cannot be easily replicated using the PySpark API. Continue reading to learn more.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>All contributions are welcome, from reporting bugs to implementing new features. Read our contributing guide to learn more.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"},{"location":"api/api-reference/","title":"Navigating Tidy Tools","text":""},{"location":"api/core/filter/","title":"Filter","text":""},{"location":"api/core/filter/#tidy_tools.core.filter.filter_nulls","title":"filter_nulls","text":"<pre><code>filter_nulls(self: DataFrame, *columns: ColumnReference, strict: bool = False, invert: bool = False) -&gt; DataFrame\n</code></pre> <p>Keep all observations that represent null across any/all column(s).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>DataFrame</code> <p>Object inheriting from PySpark DataFrame.</p> required <code>*columns</code> <code>ColumnReference</code> <p>Arbitrary number of column references. All columns must exist in <code>self</code>. If none are passed, all columns are used in filter.</p> <code>()</code> <code>strict</code> <code>bool</code> <p>Should condition be true for all column(s)?</p> <code>False</code> <code>invert</code> <code>bool</code> <p>Should observations that meet condition be kept (False) or removed (True)?</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Observations that represent null across any/all column(s).</p> Source code in <code>src/tidy_tools/core/filter.py</code> <pre><code>def filter_nulls(\n    self: DataFrame,\n    *columns: ColumnReference,\n    strict: bool = False,\n    invert: bool = False,\n) -&gt; DataFrame:  # numpydoc ignore=PR09\n    \"\"\"\n    Keep all observations that represent null across any/all column(s).\n\n    Parameters\n    ----------\n    self : DataFrame\n        Object inheriting from PySpark DataFrame.\n    *columns : ColumnReference\n        Arbitrary number of column references. All columns must exist in `self`. If none\n        are passed, all columns are used in filter.\n    strict : bool\n        Should condition be true for all column(s)?\n    invert : bool\n        Should observations that meet condition be kept (False) or removed (True)?\n\n    Returns\n    -------\n    DataFrame\n        Observations that represent null across any/all column(s).\n    \"\"\"\n    query = construct_query(\n        *columns or self.columns,\n        predicate=_predicate.is_null,\n        strict=strict,\n        invert=invert,\n    )\n    return self.filter(query)\n</code></pre>"},{"location":"api/core/filter/#tidy_tools.core.filter.filter_substring","title":"filter_substring","text":"<pre><code>filter_substring(self: DataFrame, *columns: ColumnReference, substring: str, strict: bool = False, invert: bool = False) -&gt; DataFrame\n</code></pre> <p>Keep all observations that match the regular expression across any/all column(s).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>DataFrame</code> <p>Object inheriting from PySpark DataFrame.</p> required <code>*columns</code> <code>ColumnReference</code> <p>Arbitrary number of column references. All columns must exist in <code>self</code>. If none are passed, all columns are used in filter.</p> <code>()</code> <code>substring</code> <code>str</code> <p>String expression to check.</p> required <code>strict</code> <code>bool</code> <p>Should condition be true for all column(s)?</p> <code>False</code> <code>invert</code> <code>bool</code> <p>Should observations that meet condition be kept (False) or removed (True)?</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Observations that match the substring across any/all column(s).</p> Source code in <code>src/tidy_tools/core/filter.py</code> <pre><code>def filter_substring(\n    self: DataFrame,\n    *columns: ColumnReference,\n    substring: str,\n    strict: bool = False,\n    invert: bool = False,\n) -&gt; DataFrame:  # numpydoc ignore=PR09\n    \"\"\"\n    Keep all observations that match the regular expression across any/all column(s).\n\n    Parameters\n    ----------\n    self : DataFrame\n        Object inheriting from PySpark DataFrame.\n    *columns : ColumnReference\n        Arbitrary number of column references. All columns must exist in `self`. If none\n        are passed, all columns are used in filter.\n    substring : str\n        String expression to check.\n    strict : bool\n        Should condition be true for all column(s)?\n    invert : bool\n        Should observations that meet condition be kept (False) or removed (True)?\n\n    Returns\n    -------\n    DataFrame\n        Observations that match the substring across any/all column(s).\n    \"\"\"\n    query = construct_query(\n        *columns or self.columns,\n        predicate=_predicate.is_substring,\n        substring=substring,\n        strict=strict,\n        invert=invert,\n    )\n    return self.filter(query)\n</code></pre>"},{"location":"api/core/filter/#tidy_tools.core.filter.filter_regex","title":"filter_regex","text":"<pre><code>filter_regex(self: DataFrame, *columns: ColumnReference, pattern: str, strict: bool = False, invert: bool = False) -&gt; DataFrame\n</code></pre> <p>Keep all observations that match the regular expression across any/all column(s).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>DataFrame</code> <p>Object inheriting from PySpark DataFrame.</p> required <code>*columns</code> <code>ColumnReference</code> <p>Arbitrary number of column references. All columns must exist in <code>self</code>. If none are passed, all columns are used in filter.</p> <code>()</code> <code>pattern</code> <code>str</code> <p>Regular expression. Must be compiled according to <code>re</code> library.</p> required <code>strict</code> <code>bool</code> <p>Should condition be true for all column(s)?</p> <code>False</code> <code>invert</code> <code>bool</code> <p>Should observations that meet condition be kept (False) or removed (True)?</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Observations that match the regular expression across any/all column(s).</p> Source code in <code>src/tidy_tools/core/filter.py</code> <pre><code>def filter_regex(\n    self: DataFrame,\n    *columns: ColumnReference,\n    pattern: str,\n    strict: bool = False,\n    invert: bool = False,\n) -&gt; DataFrame:  # numpydoc ignore=PR09\n    \"\"\"\n    Keep all observations that match the regular expression across any/all column(s).\n\n    Parameters\n    ----------\n    self : DataFrame\n        Object inheriting from PySpark DataFrame.\n    *columns : ColumnReference\n        Arbitrary number of column references. All columns must exist in `self`. If none\n        are passed, all columns are used in filter.\n    pattern : str\n        Regular expression. Must be compiled according to `re` library.\n    strict : bool\n        Should condition be true for all column(s)?\n    invert : bool\n        Should observations that meet condition be kept (False) or removed (True)?\n\n    Returns\n    -------\n    DataFrame\n        Observations that match the regular expression across any/all column(s).\n    \"\"\"\n    try:\n        re.compile(pattern)\n    except Exception as e:\n        print(f\"Cannot compile {pattern=} as regular expression. Raises: '{e}'\")\n    query = construct_query(\n        *columns or self.columns,\n        predicate=_predicate.is_regex_match,\n        pattern=pattern,\n        strict=strict,\n        invert=invert,\n    )\n    return self.filter(query)\n</code></pre>"},{"location":"api/core/filter/#tidy_tools.core.filter.filter_elements","title":"filter_elements","text":"<pre><code>filter_elements(self: DataFrame, *columns: ColumnReference, elements: Sequence, strict: bool = False, invert: bool = False) -&gt; DataFrame\n</code></pre> <p>Keep all observations that exist within elements across any/all column(s).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>DataFrame</code> <p>Object inheriting from PySpark DataFrame.</p> required <code>*columns</code> <code>ColumnReference</code> <p>Arbitrary number of column references. All columns must exist in <code>self</code>. If none are passed, all columns are used in filter.</p> <code>()</code> <code>elements</code> <code>Sequence</code> <p>Collection of items expected to exist in any/all column(s).</p> required <code>strict</code> <code>bool</code> <p>Should condition be true for all column(s)?</p> <code>False</code> <code>invert</code> <code>bool</code> <p>Should observations that meet condition be kept (False) or removed (True)?</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Observations that exist within range across any/all column(s).</p> Source code in <code>src/tidy_tools/core/filter.py</code> <pre><code>def filter_elements(\n    self: DataFrame,\n    *columns: ColumnReference,\n    elements: Sequence,\n    strict: bool = False,\n    invert: bool = False,\n) -&gt; DataFrame:  # numpydoc ignore=PR09\n    \"\"\"\n    Keep all observations that exist within elements across any/all column(s).\n\n    Parameters\n    ----------\n    self : DataFrame\n        Object inheriting from PySpark DataFrame.\n    *columns : ColumnReference\n        Arbitrary number of column references. All columns must exist in `self`. If none\n        are passed, all columns are used in filter.\n    elements : Sequence\n        Collection of items expected to exist in any/all column(s).\n    strict : bool\n        Should condition be true for all column(s)?\n    invert : bool\n        Should observations that meet condition be kept (False) or removed (True)?\n\n    Returns\n    -------\n    DataFrame\n        Observations that exist within range across any/all column(s).\n    \"\"\"\n    query = construct_query(\n        *columns or self.columns,\n        predicate=_predicate.is_member,\n        elements=elements,\n        strict=strict,\n        invert=invert,\n    )\n    return self.filter(query)\n</code></pre>"},{"location":"api/core/filter/#tidy_tools.core.filter.filter_range","title":"filter_range","text":"<pre><code>filter_range(self: DataFrame, *columns: ColumnReference, boundaries: Sequence[Any], strict: bool = False, invert: bool = False) -&gt; DataFrame\n</code></pre> <p>Keep all observations that exist within range across any/all column(s).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>DataFrame</code> <p>Object inheriting from PySpark DataFrame.</p> required <code>*columns</code> <code>ColumnReference</code> <p>Arbitrary number of column references. All columns must exist in <code>self</code>. If none are passed, all columns are used in filter.</p> <code>()</code> <code>boundaries</code> <code>Sequence[Any]</code> <p>Bounds of range. Must be of same type and in ascending order.</p> required <code>strict</code> <code>bool</code> <p>Should condition be true for all column(s)?</p> <code>False</code> <code>invert</code> <code>bool</code> <p>Should observations that meet condition be kept (False) or removed (True)?</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Observations that exist within range across any/all column(s).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>Raises error if either condition is not met:     - <code>lower_bound</code> is not same type as <code>upper_bound</code>     - <code>lower_bound</code> is greater than or equal to <code>upper_bound</code>.</p> Source code in <code>src/tidy_tools/core/filter.py</code> <pre><code>def filter_range(\n    self: DataFrame,\n    *columns: ColumnReference,\n    boundaries: Sequence[Any],\n    strict: bool = False,\n    invert: bool = False,\n) -&gt; DataFrame:  # numpydoc ignore=PR09\n    \"\"\"\n    Keep all observations that exist within range across any/all column(s).\n\n    Parameters\n    ----------\n    self : DataFrame\n        Object inheriting from PySpark DataFrame.\n    *columns : ColumnReference\n        Arbitrary number of column references. All columns must exist in `self`. If none\n        are passed, all columns are used in filter.\n    boundaries : Sequence[Any]\n        Bounds of range. Must be of same type and in ascending order.\n    strict : bool\n        Should condition be true for all column(s)?\n    invert : bool\n        Should observations that meet condition be kept (False) or removed (True)?\n\n    Returns\n    -------\n    DataFrame\n        Observations that exist within range across any/all column(s).\n\n    Raises\n    ------\n    AssertionError\n        Raises error if either condition is not met:\n            - `lower_bound` is not same type as `upper_bound`\n            - `lower_bound` is greater than or equal to `upper_bound`.\n    \"\"\"\n    try:\n        lower_bound, upper_bound = boundaries\n        assert type(lower_bound) is type(upper_bound)\n        assert lower_bound &lt; upper_bound\n    except AssertionError:\n        raise AssertionError(\n            f\"Boundaries must be same type and in ascending order. Received ({lower_bound=} ({type(lower_bound)}), {upper_bound=} ({type(upper_bound)}))\"\n        )\n    query = construct_query(\n        *columns or self.columns,\n        predicate=_predicate.is_between,\n        boundaries=boundaries,\n        strict=strict,\n        invert=invert,\n    )\n    return self.filter(query)\n</code></pre>"},{"location":"api/core/filter/#tidy_tools.core.filter.filter_custom","title":"filter_custom","text":"<pre><code>filter_custom(self: DataFrame, *columns: ColumnReference, predicate: Callable, strict: bool = False, invert: bool = False, **kwargs: dict) -&gt; DataFrame\n</code></pre> <p>Keep all observations that match the regular expression across any/all column(s).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>DataFrame</code> <p>Object inheriting from PySpark DataFrame.</p> required <code>*columns</code> <code>ColumnReference</code> <p>Arbitrary number of column references. All columns must exist in <code>self</code>. If none are passed, all columns are used in filter.</p> <code>()</code> <code>predicate</code> <code>Callable</code> <p>Function returning PySpark Column for filtering expression.</p> required <code>strict</code> <code>bool</code> <p>Should condition be true for all column(s)?</p> <code>False</code> <code>invert</code> <code>bool</code> <p>Should observations that meet condition be kept (False) or removed (True)?</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional options to pass to <code>predicate</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Observations that match the substring across any/all column(s).</p> Source code in <code>src/tidy_tools/core/filter.py</code> <pre><code>def filter_custom(\n    self: DataFrame,\n    *columns: ColumnReference,\n    predicate: Callable,\n    strict: bool = False,\n    invert: bool = False,\n    **kwargs: dict,\n) -&gt; DataFrame:  # numpydoc ignore=PR09\n    \"\"\"\n    Keep all observations that match the regular expression across any/all column(s).\n\n    Parameters\n    ----------\n    self : DataFrame\n        Object inheriting from PySpark DataFrame.\n    *columns : ColumnReference\n        Arbitrary number of column references. All columns must exist in `self`. If none\n        are passed, all columns are used in filter.\n    predicate : Callable\n        Function returning PySpark Column for filtering expression.\n    strict : bool\n        Should condition be true for all column(s)?\n    invert : bool\n        Should observations that meet condition be kept (False) or removed (True)?\n    **kwargs : dict, optional\n        Additional options to pass to `predicate`.\n\n    Returns\n    -------\n    DataFrame\n        Observations that match the substring across any/all column(s).\n    \"\"\"\n    query = construct_query(\n        *columns or self.columns,\n        predicate=predicate,\n        strict=strict,\n        invert=invert,\n        **kwargs,\n    )\n    return self.filter(query)\n</code></pre>"},{"location":"api/core/selector/","title":"Selector","text":""},{"location":"api/core/selector/#tidy_tools.core.selector.ColumnSelector","title":"ColumnSelector","text":"<p>Define generic class for selecting columns based on expressions.</p>"},{"location":"api/core/selector/#tidy_tools.core.selector.string","title":"string","text":"<pre><code>string() -&gt; ColumnSelector\n</code></pre> <p>Select all columns with a string dtype.</p> <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Predicate to filter columns.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def string() -&gt; ColumnSelector:\n    \"\"\"\n    Select all columns with a string dtype.\n\n    Returns\n    -------\n    ColumnSelector\n        Predicate to filter columns.\n    \"\"\"\n    return _dtype_selector(PySparkTypes.STRING.value)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.numeric","title":"numeric","text":"<pre><code>numeric() -&gt; ColumnSelector\n</code></pre> <p>Select all columns with a numeric dtype.</p> <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Predicate to filter columns.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def numeric() -&gt; ColumnSelector:\n    \"\"\"\n    Select all columns with a numeric dtype.\n\n    Returns\n    -------\n    ColumnSelector\n        Predicate to filter columns.\n    \"\"\"\n    return _dtype_selector(PySparkTypes.NUMERIC.value)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.temporal","title":"temporal","text":"<pre><code>temporal() -&gt; ColumnSelector\n</code></pre> <p>Select all columns with a temporal dtype.</p> <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Predicate to filter columns.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def temporal() -&gt; ColumnSelector:\n    \"\"\"\n    Select all columns with a temporal dtype.\n\n    Returns\n    -------\n    ColumnSelector\n        Predicate to filter columns.\n    \"\"\"\n    return _dtype_selector(PySparkTypes.TEMPORAL.value)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.date","title":"date","text":"<pre><code>date() -&gt; ColumnSelector\n</code></pre> <p>Select all columns with a date dtype.</p> <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Predicate to filter columns.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def date() -&gt; ColumnSelector:\n    \"\"\"\n    Select all columns with a date dtype.\n\n    Returns\n    -------\n    ColumnSelector\n        Predicate to filter columns.\n    \"\"\"\n    return _dtype_selector(T.DateType)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.time","title":"time","text":"<pre><code>time() -&gt; ColumnSelector\n</code></pre> <p>Select all columns with a time dtype.</p> <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Predicate to filter columns.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def time() -&gt; ColumnSelector:\n    \"\"\"\n    Select all columns with a time dtype.\n\n    Returns\n    -------\n    ColumnSelector\n        Predicate to filter columns.\n    \"\"\"\n    return _dtype_selector((T.TimestampType, T.TimestampNTZType))\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.interval","title":"interval","text":"<pre><code>interval() -&gt; ColumnSelector\n</code></pre> <p>Select all columns with an interval dtype.</p> <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Predicate to filter columns.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def interval() -&gt; ColumnSelector:\n    \"\"\"\n    Select all columns with an interval dtype.\n\n    Returns\n    -------\n    ColumnSelector\n        Predicate to filter columns.\n    \"\"\"\n    return _dtype_selector(PySparkTypes.INTERVAL.value)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.complex","title":"complex","text":"<pre><code>complex() -&gt; ColumnSelector\n</code></pre> <p>Select all columns with a complex dtype.</p> <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Predicate to filter columns.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def complex() -&gt; ColumnSelector:\n    \"\"\"\n    Select all columns with a complex dtype.\n\n    Returns\n    -------\n    ColumnSelector\n        Predicate to filter columns.\n    \"\"\"\n    return _dtype_selector(PySparkTypes.COMPLEX.value)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.by_dtype","title":"by_dtype","text":"<pre><code>by_dtype(*dtype: DataType) -&gt; Callable\n</code></pre> <p>Select all columns with dtype(s).</p> <p>Parameters:</p> Name Type Description Default <code>*dtype</code> <code>DataType</code> <p>One or more data types to filter for.</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable</code> <p>ColumnSelector predicate filtering for <code>dtype</code>.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def by_dtype(*dtype: T.DataType) -&gt; Callable:\n    \"\"\"\n    Select all columns with dtype(s).\n\n    Parameters\n    ----------\n    *dtype : T.DataType\n        One or more data types to filter for.\n\n    Returns\n    -------\n    Callable\n        ColumnSelector predicate filtering for `dtype`.\n    \"\"\"\n    return _dtype_selector(dtype)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.required","title":"required","text":"<pre><code>required() -&gt; ColumnSelector\n</code></pre> <p>Return all non-nullable fields.</p> <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Predicate-based column selecting function.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def required() -&gt; ColumnSelector:\n    \"\"\"\n    Return all non-nullable fields.\n\n    Returns\n    -------\n    ColumnSelector\n        Predicate-based column selecting function.\n    \"\"\"\n\n    def closure(sf: T.StructField) -&gt; bool:\n        \"\"\"\n        Construct StructField filtering function.\n\n        Parameters\n        ----------\n        sf : T.StructField\n            PySpark StructField.\n\n        Returns\n        -------\n        bool\n            Asserts whether field is not nullable.\n        \"\"\"\n        return not sf.nullable\n\n    return ColumnSelector(expression=closure)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.exclude","title":"exclude","text":"<pre><code>exclude(*name: str) -&gt; ColumnSelector\n</code></pre> <p>Remove all columns with <code>name</code>(s).</p> <p>Parameters:</p> Name Type Description Default <code>*name</code> <code>str</code> <p>Name of column(s) to exclude.</p> <code>()</code> <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>ColumnSelector predciate filtering for <code>dtype</code>.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def exclude(*name: str) -&gt; ColumnSelector:\n    \"\"\"\n    Remove all columns with `name`(s).\n\n    Parameters\n    ----------\n    *name : str\n        Name of column(s) to exclude.\n\n    Returns\n    -------\n    ColumnSelector\n        ColumnSelector predciate filtering for `dtype`.\n    \"\"\"\n\n    def closure(sf: T.StructField) -&gt; bool:\n        \"\"\"\n        Construct StructField filtering function.\n\n        Parameters\n        ----------\n        sf : T.StructField\n            PySpark StructField.\n\n        Returns\n        -------\n        bool\n            Asserts whether field is not in `name`.\n        \"\"\"\n        return sf.name not in name\n\n    return ColumnSelector(expression=closure)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.matches","title":"matches","text":"<pre><code>matches(pattern: str) -&gt; ColumnSelector\n</code></pre> <p>Selector capturing column names matching the pattern specified.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression to match against a column's name.</p> required <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Expression filtering for column matching <code>pattern</code>.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def matches(pattern: str) -&gt; ColumnSelector:\n    \"\"\"\n    Selector capturing column names matching the pattern specified.\n\n    Parameters\n    ----------\n    pattern : str\n        Regular expression to match against a column's name.\n\n    Returns\n    -------\n    ColumnSelector\n        Expression filtering for column matching `pattern`.\n    \"\"\"\n    return _name_selector(\n        pattern=re.compile(pattern),\n        match_func=lambda name, pattern: re.search(\n            re.compile(pattern), name\n        ),  # swap order of parameters for _name_selector.closure\n    )\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.contains","title":"contains","text":"<pre><code>contains(pattern: str) -&gt; ColumnSelector\n</code></pre> <p>Selector capturing column names containing the exact pattern specified.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression to match against a column's name.</p> required <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Expression filtering for column containing <code>pattern</code>.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def contains(pattern: str) -&gt; ColumnSelector:\n    \"\"\"\n    Selector capturing column names containing the exact pattern specified.\n\n    Parameters\n    ----------\n    pattern : str\n        Regular expression to match against a column's name.\n\n    Returns\n    -------\n    ColumnSelector\n        Expression filtering for column containing `pattern`.\n    \"\"\"\n    return _name_selector(pattern=pattern, match_func=str.__contains__)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.starts_with","title":"starts_with","text":"<pre><code>starts_with(pattern: str) -&gt; ColumnSelector\n</code></pre> <p>Selector capturing column names starting with the exact pattern specified.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression to match against a column's name.</p> required <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Expression filtering for column starting with <code>pattern</code>.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def starts_with(pattern: str) -&gt; ColumnSelector:\n    \"\"\"\n    Selector capturing column names starting with the exact pattern specified.\n\n    Parameters\n    ----------\n    pattern : str\n        Regular expression to match against a column's name.\n\n    Returns\n    -------\n    ColumnSelector\n        Expression filtering for column starting with `pattern`.\n    \"\"\"\n    return _name_selector(pattern=pattern, match_func=str.startswith)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.ends_with","title":"ends_with","text":"<pre><code>ends_with(pattern: str) -&gt; ColumnSelector\n</code></pre> <p>Selector capturing column names ending with the exact pattern specified.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression to match against a column's name.</p> required <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Expression filtering for column ending with <code>pattern</code>.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def ends_with(pattern: str) -&gt; ColumnSelector:\n    \"\"\"\n    Selector capturing column names ending with the exact pattern specified.\n\n    Parameters\n    ----------\n    pattern : str\n        Regular expression to match against a column's name.\n\n    Returns\n    -------\n    ColumnSelector\n        Expression filtering for column ending with `pattern`.\n    \"\"\"\n    return _name_selector(pattern=pattern, match_func=str.endswith)\n</code></pre>"},{"location":"api/core/selector/#tidy_tools.core.selector.by_name","title":"by_name","text":"<pre><code>by_name(*name: str) -&gt; ColumnSelector\n</code></pre> <p>Selector capturing column(s) by name.</p> <p>Parameters:</p> Name Type Description Default <code>*name</code> <code>str</code> <p>Name of column(s) to select.</p> <code>()</code> <p>Returns:</p> Type Description <code>ColumnSelector</code> <p>Expression filtering for columns with <code>name</code>.</p> Source code in <code>src/tidy_tools/core/selector.py</code> <pre><code>def by_name(*name: str) -&gt; ColumnSelector:\n    \"\"\"\n    Selector capturing column(s) by name.\n\n    Parameters\n    ----------\n    *name : str\n        Name of column(s) to select.\n\n    Returns\n    -------\n    ColumnSelector\n        Expression filtering for columns with `name`.\n    \"\"\"\n    return matches(pattern=rf\"^({'|'.join(name)})$\")\n</code></pre>"},{"location":"api/dataframe/context/","title":"Context","text":""},{"location":"api/dataframe/context/#tidy_tools.dataframe.context.TidyContext","title":"TidyContext","text":"<p>Parameters supported by TidyDataFrame contextual operations.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of DataFrame.</p> <code>count</code> <code>bool</code> <p>Whether to perform count operations.</p> <code>display</code> <code>bool</code> <p>Whether to perform display operations.</p> <code>limit</code> <code>int</code> <p>Default all display operations to display only <code>limit</code> rows.</p> <code>log_handlers</code> <code>list[TidyLogHandler]</code> <p>Sequence of TidyLogHandler instances to configure for TidyDataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # assuming PySpark DataFrame is loaded\n&gt;&gt;&gt; spark_data = ...\n&gt;&gt;&gt;\n&gt;&gt;&gt; # default configuration\n&gt;&gt;&gt; default_context = TidyContext()\n&gt;&gt;&gt; default_dataframe = TidyDataFrame(spark_data, default_context)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # simple contextual configuration\n&gt;&gt;&gt; basic_context = TidyContext(\n&gt;&gt;&gt;     name=\"ContextDataFrame\",\n&gt;&gt;&gt;     count=False,\n&gt;&gt;&gt;     limit=10\n&gt;&gt;&gt; )\n&gt;&gt;&gt; basic_dataframe = TidyDataFrame(spark_data, basic_context)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # attaching log handlers\n&gt;&gt;&gt; logging_context = TidyContext(\n&gt;&gt;&gt;     name=\"LoggingHandlers\",\n&gt;&gt;&gt;     log_handlers=[\n&gt;&gt;&gt;         TidyLogHandler(),\n&gt;&gt;&gt;         TidyFileHandler(\"example.log\"),\n&gt;&gt;&gt;         TidyMemoHandler(\"serialized_example.log\")\n&gt;&gt;&gt;     ]\n&gt;&gt;&gt; )\n&gt;&gt;&gt; logging_dataframe = TidyDataFrame(spark_data, logging_context)\n</code></pre>"},{"location":"api/dataframe/context/#tidy_tools.dataframe.context.TidyContext.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(context: str | Path | dict) -&gt; TidyContext\n</code></pre> <p>Create TidyContext from pre-configured context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str | Path | dict</code> <p>Reference to object containing TidyContext attributes. If <code>str</code> or <code>Path</code>, the contents are loaded from the path provided. Once parsed from the path (or passed if a <code>dict</code>), a new TidyContext instance will be created.</p> required <p>Returns:</p> Type Description <code>TidyContext</code> <p>Instance of TidyContext configured with provided parameters.</p> Source code in <code>src/tidy_tools/dataframe/context.py</code> <pre><code>@classmethod\ndef load(cls, context: str | Path | dict) -&gt; \"TidyContext\":\n    \"\"\"\n    Create TidyContext from pre-configured context.\n\n    Parameters\n    ----------\n    context : str | Path | dict\n        Reference to object containing TidyContext attributes. If `str` or\n        `Path`, the contents are loaded from the path provided. Once parsed\n        from the path (or passed if a `dict`), a new TidyContext instance\n        will be created.\n\n    Returns\n    -------\n    TidyContext\n        Instance of TidyContext configured with provided parameters.\n    \"\"\"\n    if isinstance(context, (str, Path)):\n        with open(context, \"r\") as fp:\n            context = json.load(fp)\n    return TidyContext(**context)\n</code></pre>"},{"location":"api/dataframe/context/#tidy_tools.dataframe.context.TidyContext.save","title":"save","text":"<pre><code>save(filepath: str | Path | None = None) -&gt; dict | None\n</code></pre> <p>Save attributes as serialized object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path | None</code> <p>Optional path to save context configuration. This file can be loaded using the <code>TidyContext.load(&lt;filepath&gt;)</code> method to deterministically create copies of the same instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict | None</code> <p>If no <code>filepath</code> is provided, the attributes of <code>TidyContext</code> instance as dictionary. Else, write configurations to <code>filepath</code>.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error while writing to <code>filepath</code>.</p> Source code in <code>src/tidy_tools/dataframe/context.py</code> <pre><code>def save(self, filepath: str | Path | None = None) -&gt; dict | None:\n    \"\"\"\n    Save attributes as serialized object.\n\n    Parameters\n    ----------\n    filepath : str | Path | None\n        Optional path to save context configuration. This file can be\n        loaded using the `TidyContext.load(&lt;filepath&gt;)` method to\n        deterministically create copies of the same instance.\n\n    Returns\n    -------\n    dict | None\n        If no `filepath` is provided, the attributes of `TidyContext`\n        instance as dictionary. Else, write configurations to `filepath`.\n\n    Raises\n    ------\n    Exception\n        If there is an error while writing to `filepath`.\n    \"\"\"\n    context = attrs.asdict(self)\n    if filepath is None:\n        return context\n    if not isinstance(filepath, Path):\n        filepath = Path(filepath).resolve()\n    try:\n        with open(filepath, \"w\") as fp:\n            json.dump(self.save(), fp)\n        logger.success(f\"Context stored at: {filepath}\")\n    except Exception as e:\n        logger.error(f\"Error writing context to: {filepath}\")\n        raise e\n</code></pre>"},{"location":"api/dataframe/dataframe/","title":"Dataframe","text":"<p>Enable tidy operations on a PySpark DataFrame with optional context.</p> <p>TidyDataFrame is a PySpark DataFrame with built-in logging functionality. Directly inspired by the tidylog project, TidyDataFrame decorates common DataFrame methods to detail the impact of said method in real-time. Combined with the context to control other behavior (e.g. disabling displays, logging to multiple handlers), TidyDataFrame is the all-in-one logging solution for PySpark workflows.</p> <p>Attributes:</p> Name Type Description <code>_data</code> <code>DataFrame</code> <p>PySpark DataFrame object to perform tidy operations.</p> <code>_context</code> <code>TidyContext | None</code> <p>Context to control execution of TidyDataFrame. See <code>TidyContext</code> for more.</p>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.columns","title":"columns  <code>property</code>","text":"<pre><code>columns\n</code></pre> <p>Return the raw Spark DataFrame.</p>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.dtypes","title":"dtypes  <code>property</code>","text":"<pre><code>dtypes\n</code></pre> <p>Return all column names and data types as a list.</p>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.describe","title":"describe  <code>property</code>","text":"<pre><code>describe\n</code></pre> <p>Compute basic statistics for numeric and string columns.</p>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.schema","title":"schema  <code>property</code>","text":"<pre><code>schema\n</code></pre> <p>Return schema as a pyspark.sql.types.StructType object.</p>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.data","title":"data  <code>property</code>","text":"<pre><code>data\n</code></pre> <p>Return the raw Spark DataFrame.</p>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.comment","title":"comment","text":"<pre><code>comment(message: str) -&gt; None\n</code></pre> <p>Log message to console to persist comments in logs.</p> Source code in <code>src/tidy_tools/dataframe/dataframe.py</code> <pre><code>def comment(self, message: str) -&gt; None:  # numpydoc ignore=PR01,RT01\n    \"\"\"Log message to console to persist comments in logs.\"\"\"\n    self._log(operation=\"comment\", message=message)\n    return self\n</code></pre>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.document","title":"document","text":"<pre><code>document(message: str) -&gt; None\n</code></pre> <p>Log message to console for documentation purposes.</p> Source code in <code>src/tidy_tools/dataframe/dataframe.py</code> <pre><code>def document(self, message: str) -&gt; None:  # numpydoc ignore=PR01,RT01\n    \"\"\"Log message to console for documentation purposes.\"\"\"\n    self._log(operation=\"document\", message=message)\n    return self\n</code></pre>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(*source: str | Path, context: TidyContext | None = None, read_func: Callable | None = None, read_options: dict | None = dict()) -&gt; TidyDataFrame\n</code></pre> <p>Create TidyDataFrame directly from source(s).</p> <p>Parameters:</p> Name Type Description Default <code>*source</code> <code>str | Path</code> <p>Arbitrary number of file references containing data.</p> <code>()</code> <code>context</code> <code>TidyContext | None</code> <p>Additional context parameters to pass.</p> <code>None</code> <code>read_func</code> <code>Callable | None</code> <p>Function for reading data from source(s).</p> <code>None</code> <code>read_options</code> <code>dict | None</code> <p>Additional parameters to pass to <code>read_func</code>.</p> <code>dict()</code> <p>Returns:</p> Type Description <code>TidyDataFrame</code> <p>Instance of TidyDataFrame loaded from source(s) with additional parameters instructing read-in and/or context.</p> <p>Raises:</p> Type Description <code>PySparkException</code> <p>If <code>reader.read</code> cannot load data from source(s).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # load data from a single source\n&gt;&gt;&gt; tidy_data = TidyDataFrame.load(\"path/to/data.csv\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # load data from multiple sources\n&gt;&gt;&gt; tidy_data = TidyDataFrame.load(\n&gt;&gt;&gt;     \"path/to/data.csv\",\n&gt;&gt;&gt;     \"path/to/another/file.txt\",\n&gt;&gt;&gt;     \"path/to/the/final/file.xlsx\",\n&gt;&gt;&gt; )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # load data with context\n&gt;&gt;&gt; tidy_data = TidyDataFrame.load(..., context=TidyContext(...))\n&gt;&gt;&gt;\n&gt;&gt;&gt; # load data with read-in instructions\n&gt;&gt;&gt; tidy_data = TidyDataFrame.load(\n&gt;&gt;&gt;     ...,\n&gt;&gt;&gt;     read_func=spark.read.csv,\n&gt;&gt;&gt;     read_options={\"header\": \"true\"}\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>src/tidy_tools/dataframe/dataframe.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    *source: str | Path,\n    context: TidyContext | None = None,\n    read_func: Callable | None = None,\n    read_options: dict | None = dict(),\n) -&gt; \"TidyDataFrame\":\n    \"\"\"\n    Create TidyDataFrame directly from source(s).\n\n    Parameters\n    ----------\n    *source : str | Path\n        Arbitrary number of file references containing data.\n    context : TidyContext | None\n        Additional context parameters to pass.\n    read_func : Callable | None\n        Function for reading data from source(s).\n    read_options : dict | None\n        Additional parameters to pass to `read_func`.\n\n    Returns\n    -------\n    TidyDataFrame\n        Instance of TidyDataFrame loaded from source(s) with additional\n        parameters instructing read-in and/or context.\n\n    Raises\n    ------\n    PySparkException\n        If `reader.read` cannot load data from source(s).\n\n    Examples\n    --------\n    &gt;&gt;&gt; # load data from a single source\n    &gt;&gt;&gt; tidy_data = TidyDataFrame.load(\"path/to/data.csv\")\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # load data from multiple sources\n    &gt;&gt;&gt; tidy_data = TidyDataFrame.load(\n    &gt;&gt;&gt;     \"path/to/data.csv\",\n    &gt;&gt;&gt;     \"path/to/another/file.txt\",\n    &gt;&gt;&gt;     \"path/to/the/final/file.xlsx\",\n    &gt;&gt;&gt; )\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # load data with context\n    &gt;&gt;&gt; tidy_data = TidyDataFrame.load(..., context=TidyContext(...))\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # load data with read-in instructions\n    &gt;&gt;&gt; tidy_data = TidyDataFrame.load(\n    &gt;&gt;&gt;     ...,\n    &gt;&gt;&gt;     read_func=spark.read.csv,\n    &gt;&gt;&gt;     read_options={\"header\": \"true\"}\n    &gt;&gt;&gt; )\n    \"\"\"\n    try:\n        read_func = functools.partial(read_func, **read_options)\n        data = reader.read(*source, read_func=read_func)\n        if context:\n            return TidyDataFrame(data, context)\n        return TidyDataFrame(data)\n    except PySparkException as e:\n        raise e\n</code></pre>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.is_empty","title":"is_empty","text":"<pre><code>is_empty()\n</code></pre> <p>Check if data is empty.</p> Source code in <code>src/tidy_tools/dataframe/dataframe.py</code> <pre><code>def is_empty(self):  # numpydoc ignore=RT01\n    \"\"\"Check if data is empty.\"\"\"\n    return self._data.isEmpty()\n</code></pre>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.display","title":"display","text":"<pre><code>display(limit: int | None = None) -&gt; None\n</code></pre> <p>Control execution of display method.</p> <p>This method masks the <code>pyspark.sql.DataFrame.display</code> method. This method does not mask the native PySpark display function.</p> <p>Often, the <code>.display()</code> method will need to be disabled for logging purposes. Similar to toggling the <code>.count()</code> method, users can temporarily disable a DataFrame's ability to display to the console by passing <code>toggle_display = True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Number of rows to display to console. If context is provided, the limit provided will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Displays data to console or nothing if display is disabled.</p> Source code in <code>src/tidy_tools/dataframe/dataframe.py</code> <pre><code>def display(self, limit: int | None = None) -&gt; None:\n    \"\"\"\n    Control execution of display method.\n\n    This method masks the `pyspark.sql.DataFrame.display` method. This method does not\n    mask the native PySpark display function.\n\n    Often, the `.display()` method will need to be disabled for logging purposes. Similar\n    to toggling the `.count()` method, users can temporarily disable a DataFrame's\n    ability to display to the console by passing `toggle_display = True`.\n\n    Parameters\n    ----------\n    limit : int | None\n        Number of rows to display to console. If context is provided, the limit provided\n        will be used.\n\n    Returns\n    -------\n    None\n        Displays data to console or nothing if display is disabled.\n    \"\"\"\n    if not self._context.display:\n        self._log(\n            operation=\"display\", message=\"display is toggled off\", level=\"warning\"\n        )\n    else:\n        self._data.limit(limit or self._context.limit).display()\n    return self\n</code></pre>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.show","title":"show","text":"<pre><code>show(limit: int | None = None) -&gt; None\n</code></pre> <p>Control execution of display method.</p> <p>This method masks the <code>pyspark.sql.DataFrame.display</code> method. This method does not mask the native PySpark display function.</p> <p>Often, the <code>.display()</code> method will need to be disabled for logging purposes. Similar to toggling the <code>.count()</code> method, users can temporarily disable a DataFrame's ability to display to the console by passing <code>toggle_display = True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int | None</code> <p>Number of rows to display to console. If context is provided, the limit provided will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Displays data to console or nothing if display is disabled.</p> Source code in <code>src/tidy_tools/dataframe/dataframe.py</code> <pre><code>def show(self, limit: int | None = None) -&gt; None:\n    \"\"\"\n    Control execution of display method.\n\n    This method masks the `pyspark.sql.DataFrame.display` method. This method does not\n    mask the native PySpark display function.\n\n    Often, the `.display()` method will need to be disabled for logging purposes. Similar\n    to toggling the `.count()` method, users can temporarily disable a DataFrame's\n    ability to display to the console by passing `toggle_display = True`.\n\n    Parameters\n    ----------\n    limit : int | None\n        Number of rows to display to console. If context is provided, the limit provided\n        will be used.\n\n    Returns\n    -------\n    None\n        Displays data to console or nothing if display is disabled.\n    \"\"\"\n    if not self._context.display:\n        self._log(\n            operation=\"display\", message=\"display is toggled off\", level=\"warning\"\n        )\n    else:\n        self._data.limit(limit or self._context.limit).show()\n    return self\n</code></pre>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.count","title":"count","text":"<pre><code>count(result: DataFrame | None = None) -&gt; int\n</code></pre> <p>Return number of rows in DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>DataFrame | None</code> <p>If provided, this will trigger a count operation. Else, the count will reference the last count or zero if context disables count.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of rows in data or zero if count is disabled in context.</p> Source code in <code>src/tidy_tools/dataframe/dataframe.py</code> <pre><code>def count(self, result: DataFrame | None = None) -&gt; int:\n    \"\"\"\n    Return number of rows in DataFrame.\n\n    Parameters\n    ----------\n    result : DataFrame | None\n        If provided, this will trigger a count operation. Else, the count will reference\n        the last count or zero if context disables count.\n\n    Returns\n    -------\n    int\n        Number of rows in data or zero if count is disabled in context.\n    \"\"\"\n    if not self._context.count:\n        return 0\n    if result:\n        return result._data.count()\n    return self._data.count()\n</code></pre>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.transform","title":"transform","text":"<pre><code>transform(func: Callable, *args: tuple, **kwargs: dict) -&gt; TidyDataFrame\n</code></pre> <p>Concise syntax for chaining custom transformations together.</p> <p>If calling multiple times in succession, consider using <code>TidyDataFrame.pipe</code>.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>Custom transformation function(s) to apply to data.</p> required <code>*args</code> <code>tuple</code> <p>Arbitrary number of positional arguments to pass to <code>func</code>.</p> <code>()</code> <code>**kwargs</code> <code>dict</code> <p>Arbitrary number of keyword arguments to pass to <code>func</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TidyDataFrame</code> <p>Transformed data.</p> Source code in <code>src/tidy_tools/dataframe/dataframe.py</code> <pre><code>def transform(\n    self, func: Callable, *args: tuple, **kwargs: dict\n) -&gt; \"TidyDataFrame\":\n    \"\"\"\n    Concise syntax for chaining custom transformations together.\n\n    If calling multiple times in succession, consider using `TidyDataFrame.pipe`.\n\n    Parameters\n    ----------\n    func : Callable\n        Custom transformation function(s) to apply to data.\n    *args : tuple\n        Arbitrary number of positional arguments to pass to `func`.\n    **kwargs : dict\n        Arbitrary number of keyword arguments to pass to `func`.\n\n    Returns\n    -------\n    TidyDataFrame\n        Transformed data.\n    \"\"\"\n    # include docstring in logs if provided\n    docstring = inspect.getdoc(func)\n    if docstring:\n        self._log(\n            operation=\"document\",\n            message=f\"{inspect.cleandoc(docstring)} ({func.__name__})\",\n        )\n\n    result = func(self, *args, **kwargs)\n    return TidyDataFrame(result._data, self._context)\n</code></pre>"},{"location":"api/dataframe/dataframe/#tidy_tools.dataframe.TidyDataFrame.pipe","title":"pipe","text":"<pre><code>pipe(*funcs: Callable) -&gt; TidyDataFrame\n</code></pre> <p>Iteratively apply custom transformation functions.</p> <p>Functional alias for <code>TidyDataFrame.transform</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*funcs</code> <code>Callable</code> <p>Custom transformation function(s) to apply to data.</p> <code>()</code> <p>Returns:</p> Type Description <code>TidyDataFrame</code> <p>Transformed data.</p> Source code in <code>src/tidy_tools/dataframe/dataframe.py</code> <pre><code>def pipe(self, *funcs: Callable) -&gt; \"TidyDataFrame\":\n    \"\"\"\n    Iteratively apply custom transformation functions.\n\n    Functional alias for `TidyDataFrame.transform`.\n\n    Parameters\n    ----------\n    *funcs : Callable\n        Custom transformation function(s) to apply to data.\n\n    Returns\n    -------\n    TidyDataFrame\n        Transformed data.\n    \"\"\"\n    result = functools.reduce(lambda init, func: init.transform(func), funcs, self)\n    return TidyDataFrame(result._data, self._context)\n</code></pre>"},{"location":"api/dataframe/handler/","title":"Handler","text":""},{"location":"api/dataframe/handler/#tidy_tools.dataframe.handler.TidyLogHandler","title":"TidyLogHandler","text":"<p>Generic log handler for system error streams.</p> <p>Attributes:</p> Name Type Description <code>sink</code> <code>str | Path | TextIO</code> <p>Destination for receiving logging messages.</p> <code>level</code> <code>str</code> <p>Minimum level to trace in logs. See <code>loguru</code> for more details.</p> <code>format</code> <code>str</code> <p>Template used for logged messages.</p> <code>diagnose</code> <code>bool</code> <p>Whether the exception trace should display the variables values to eases the debugging.</p> <code>catch</code> <code>bool</code> <p>Whether errors occurring while sink handles logs messages should be automatically caught. If True, an exception message is displayed on sys.stderr but the exception is not propagated to the caller, preventing your app to crash.</p>"},{"location":"api/dataframe/handler/#tidy_tools.dataframe.handler.TidyLogHandler.pattern","title":"pattern  <code>property</code>","text":"<pre><code>pattern: str\n</code></pre> <p>Pattern for logging instance.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Template used for parsing logged messages.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Pattern for logging instance.</p>"},{"location":"api/dataframe/handler/#tidy_tools.dataframe.handler.TidyLogHandler.save","title":"save","text":"<pre><code>save() -&gt; dict\n</code></pre> <p>Return configurations as dict.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Collection of attributes for context.</p> Source code in <code>src/tidy_tools/dataframe/handler.py</code> <pre><code>def save(self) -&gt; dict:\n    \"\"\"\n    Return configurations as dict.\n\n    Returns\n    -------\n    dict\n        Collection of attributes for context.\n    \"\"\"\n    return attrs.asdict(self)\n</code></pre>"},{"location":"api/dataframe/handler/#tidy_tools.dataframe.handler.TidyLogHandler.summarize","title":"summarize","text":"<pre><code>summarize() -&gt; dict\n</code></pre> <p>Summarize contents at <code>source</code>.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Statistics of contents at <code>source</code>.</p> Source code in <code>src/tidy_tools/dataframe/handler.py</code> <pre><code>def summarize(self) -&gt; dict:\n    \"\"\"\n    Summarize contents at `source`.\n\n    Returns\n    -------\n    dict\n        Statistics of contents at `source`.\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"api/dataframe/handler/#tidy_tools.dataframe.handler.TidyFileHandler","title":"TidyFileHandler","text":"<p>               Bases: <code>TidyLogHandler</code></p> <p>Log handler for file streams.</p>"},{"location":"api/dataframe/handler/#tidy_tools.dataframe.handler.TidyMemoHandler","title":"TidyMemoHandler","text":"<p>               Bases: <code>TidyFileHandler</code></p> <p>Log handler for serialized streams.</p> <p>Attributes:</p> Name Type Description <code>serialize</code> <code>bool</code> <p>Whether the logged message and its records should be first converted to a JSON string before being sent to the sink.</p>"},{"location":"api/functions/merge/","title":"Merge","text":""},{"location":"api/functions/merge/#tidy_tools.functions.merge.merge","title":"merge","text":"<pre><code>merge(*data: DataFrame, func: Callable, **kwargs: dict) -&gt; DataFrame\n</code></pre> <p>Merge an arbitrary number of DataFrames into a single DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>*data</code> <code>DataFrame</code> <p>PySpark DataFrame.</p> <code>()</code> <code>func</code> <code>Callable</code> <p>Reduce function to merge two DataFrames to each other. By default, this union resolves by column name.</p> required <code>**kwargs</code> <code>dict</code> <p>Keyword-arguments for merge function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result of merging all <code>data</code> objects by <code>func</code>.</p> Source code in <code>src/tidy_tools/functions/merge.py</code> <pre><code>def merge(*data: DataFrame, func: Callable, **kwargs: dict) -&gt; DataFrame:\n    \"\"\"\n    Merge an arbitrary number of DataFrames into a single DataFrame.\n\n    Parameters\n    ----------\n    *data : DataFrame\n        PySpark DataFrame.\n    func : Callable\n        Reduce function to merge two DataFrames to each other. By default, this\n        union resolves by column name.\n    **kwargs : dict, optional\n        Keyword-arguments for merge function.\n\n    Returns\n    -------\n    DataFrame\n        Result of merging all `data` objects by `func`.\n    \"\"\"\n    func = functools.partial(func, **kwargs)\n    return functools.reduce(func, data)\n</code></pre>"},{"location":"api/functions/merge/#tidy_tools.functions.merge.concat","title":"concat","text":"<pre><code>concat(*data: DataFrame, func: Callable = unionByName, **kwargs: dict) -&gt; DataFrame\n</code></pre> <p>Concatenate an aribitrary number of DataFrames into a single DataFrame.</p> <p>By default, all objects are appended to one another by column name. An error will be raised if column names do not align.</p> <p>Parameters:</p> Name Type Description Default <code>*data</code> <code>DataFrame</code> <p>PySpark DataFrame.</p> <code>()</code> <code>func</code> <code>Callable</code> <p>Reduce function to concatenate two DataFrames to each other. By default, this union resolves by column name.</p> <code>unionByName</code> <code>**kwargs</code> <code>dict</code> <p>Keyword-arguments for merge function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result of concatenating <code>data</code>.</p> Source code in <code>src/tidy_tools/functions/merge.py</code> <pre><code>def concat(\n    *data: DataFrame,\n    func: Callable = DataFrame.unionByName,\n    **kwargs: dict,\n) -&gt; DataFrame:\n    \"\"\"\n    Concatenate an aribitrary number of DataFrames into a single DataFrame.\n\n    By default, all objects are appended to one another by column name. An error\n    will be raised if column names do not align.\n\n    Parameters\n    ----------\n    *data : DataFrame\n        PySpark DataFrame.\n    func : Callable\n        Reduce function to concatenate two DataFrames to each other. By default, this\n        union resolves by column name.\n    **kwargs : dict, optional\n        Keyword-arguments for merge function.\n\n    Returns\n    -------\n    DataFrame\n        Result of concatenating `data`.\n    \"\"\"\n    return merge(*data, func=func, **kwargs)\n</code></pre>"},{"location":"api/functions/merge/#tidy_tools.functions.merge.join","title":"join","text":"<pre><code>join(*data: DataFrame, on: str | Column, how: str = 'inner', func: Callable = join, **kwargs: dict) -&gt; DataFrame\n</code></pre> <p>Join an aribitrary number of DataFrames into a single DataFrame.</p> <p>By default, all objects are appended to one another by column name. An error will be raised if column names do not align.</p> <p>Parameters:</p> Name Type Description Default <code>*data</code> <code>DataFrame</code> <p>PySpark DataFrame.</p> <code>()</code> <code>on</code> <code>str | Column</code> <p>Column name or expression to perform join.</p> required <code>how</code> <code>str</code> <p>Set operation to perform.</p> <code>'inner'</code> <code>func</code> <code>Callable</code> <p>Reduce function to join two DataFrames to each other.</p> <code>join</code> <code>**kwargs</code> <code>dict</code> <p>Keyword-arguments for merge function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Result of joining <code>data</code>.</p> Source code in <code>src/tidy_tools/functions/merge.py</code> <pre><code>def join(\n    *data: DataFrame,\n    on: str | Column,\n    how: str = \"inner\",\n    func: Callable = DataFrame.join,\n    **kwargs: dict,\n) -&gt; DataFrame:\n    \"\"\"\n    Join an aribitrary number of DataFrames into a single DataFrame.\n\n    By default, all objects are appended to one another by column name. An error\n    will be raised if column names do not align.\n\n    Parameters\n    ----------\n    *data : DataFrame\n        PySpark DataFrame.\n    on : str | Column\n        Column name or expression to perform join.\n    how : str\n        Set operation to perform.\n    func : Callable\n        Reduce function to join two DataFrames to each other.\n    **kwargs : dict, optional\n        Keyword-arguments for merge function.\n\n    Returns\n    -------\n    DataFrame\n        Result of joining `data`.\n    \"\"\"\n    return merge(*data, func=func, on=on, how=how, **kwargs)\n</code></pre>"},{"location":"api/functions/reader/","title":"Reader","text":""},{"location":"api/functions/reader/#tidy_tools.functions.reader.read","title":"read","text":"<pre><code>read(*source: str | Path | DataFrame, read_func: Callable, **read_options: dict) -&gt; DataFrame\n</code></pre> <p>Load data from source(s) as a PySpark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>*source</code> <code>str | Path | DataFrame</code> <p>Arbitrary number of data references. If file-like reference, data will be loaded using <code>read_func</code> and optional <code>read_options</code>. If DataFrame, data will be returned.</p> <code>()</code> <code>read_func</code> <code>Callable</code> <p>Function to load data from source(s).</p> required <code>**read_options</code> <code>dict</code> <p>Additional arguments to pass to the read_function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Object containing data from all source(s) provided.</p> <p>Raises:</p> Type Description <code>PySparkException</code> <p>If reading source(s) cannot be performed successfully.</p> Source code in <code>src/tidy_tools/functions/reader.py</code> <pre><code>def read(\n    *source: str | Path | DataFrame,\n    read_func: Callable,\n    **read_options: dict,\n) -&gt; DataFrame:\n    \"\"\"\n    Load data from source(s) as a PySpark DataFrame.\n\n    Parameters\n    ----------\n    *source : str | Path | DataFrame\n        Arbitrary number of data references. If file-like reference, data will\n        be loaded using `read_func` and optional `read_options`. If DataFrame,\n        data will be returned.\n    read_func : Callable\n        Function to load data from source(s).\n    **read_options : dict\n        Additional arguments to pass to the read_function.\n\n    Returns\n    -------\n    DataFrame\n        Object containing data from all source(s) provided.\n\n    Raises\n    ------\n    PySparkException\n        If reading source(s) cannot be performed successfully.\n    \"\"\"\n\n    def _read_func(source: str | Path | DataFrame) -&gt; DataFrame:\n        \"\"\"\n        Wrap read function to skip DataFrame instances.\n\n        Parameters\n        ----------\n        source : str | Path | DataFrame\n            Reference to data object.\n\n        Returns\n        -------\n        DataFrame\n            Contents of data object.\n        \"\"\"\n        if isinstance(source, DataFrame):\n            return source\n        return read_func(source, **read_options)\n\n    try:\n        logger.info(f\"Attempting to load {len(source)} source(s)\")\n        data = concat(*map(_read_func, source))\n        logger.success(f\"Loaded {data.count():,} rows.\")\n    except PySparkException as e:\n        logger.error(\"Reader failed while loading data.\")\n        raise e\n    return data\n</code></pre>"},{"location":"api/model/convert/","title":"Convert","text":"<p>Convert data according to a class schema.</p> <p>Parameters:</p> Name Type Description Default <code>cls_field</code> <code>Attribute</code> <p>Field to apply conversion function.</p> required <code>cls_field_exists</code> <code>bool</code> <p>Whether field exists in data already.</p> required <p>Returns:</p> Type Description <code>Column</code> <p>Constructed PySpark Column expression.</p> Source code in <code>src/tidy_tools/model/convert.py</code> <pre><code>def convert_field(cls_field: attrs.Attribute, cls_field_exists: bool) -&gt; Column:\n    \"\"\"\n    Convert data according to a class schema.\n\n    Parameters\n    ----------\n    cls_field : attrs.Attribute\n        Field to apply conversion function.\n    cls_field_exists : bool\n        Whether field exists in data already.\n\n    Returns\n    -------\n    Column\n        Constructed PySpark Column expression.\n    \"\"\"\n    if not cls_field.default:\n        if not cls_field_exists:\n            column = F.lit(None)\n        else:\n            column = F.col(cls_field.alias)\n\n    if cls_field.default:\n        if isinstance(cls_field.default, attrs.Factory):\n            return_type = typing.get_type_hints(cls_field.default.factory).get(\"return\")\n            assert (\n                return_type is not None\n            ), \"Missing type hint for return value! Redefine function to include type hint `def func() -&gt; pyspark.sql.Column: ...`\"\n            assert return_type is Column, \"Factory must return a pyspark.sql.Column!\"\n            column = cls_field.default.factory()\n        elif not cls_field_exists:\n            column = F.lit(cls_field.default)\n        else:\n            column = F.when(\n                F.col(cls_field.alias).isNull(), cls_field.default\n            ).otherwise(F.col(cls_field.alias))\n    else:\n        column = F.col(cls_field.alias)\n\n    if cls_field.name != cls_field.alias:\n        column = column.alias(cls_field.name)\n\n    cls_field_type = get_pyspark_type(cls_field)\n    match cls_field_type:\n        case T.DateType():\n            date_format = cls_field.metadata.get(\"format\")\n            if date_format:\n                column = F.to_date(column, format=date_format)\n            else:\n                logger.warning(\n                    f\"No `format` provided for {cls_field.name}. Please add `field(..., metadata={{'format': ...}})` and rerun.\"\n                )\n                column = column.cast(cls_field_type)\n        case T.TimestampType():\n            timestamp_format = cls_field.metadata.get(\"format\")\n            if timestamp_format:\n                column = F.to_timestamp(column, format=timestamp_format)\n            else:\n                logger.warning(\n                    f\"No `format` provided for {cls_field.name}. Please add `field(..., metadata={{'format': ...}})` and rerun.\"\n                )\n                column = column.cast(cls_field_type)\n\n    if cls_field.converter:\n        column = cls_field.converter(column)\n\n    return column.alias(cls_field.alias)\n</code></pre>"},{"location":"api/model/model/","title":"Model","text":""},{"location":"api/model/model/#tidy_tools.model.TidyDataModel.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(*source: str | Path | DataFrame, read_func: Callable, read_options: dict = dict()) -&gt; DataFrame\n</code></pre> <p>Load data from source(s) and apply processing, conversion, and validation procedures.</p> <p>See <code>TidyDataModel.tidy()</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>*source</code> <code>str | Path | DataFrame</code> <p>Arbitrary number of reference(s) to data source(s).</p> <code>()</code> <code>read_func</code> <code>Callable</code> <p>Function to load data from source(s).</p> required <code>read_options</code> <code>dict</code> <p>Keyword arguments to pass to <code>read_func</code>.</p> <code>dict()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Single DataFrame containing data from all source(s) coerced according to class schema.</p> Source code in <code>src/tidy_tools/model/model.py</code> <pre><code>@classmethod\ndef load(\n    cls,\n    *source: str | pathlib.Path | DataFrame,\n    read_func: Callable,\n    read_options: dict = dict(),\n) -&gt; DataFrame:\n    \"\"\"\n    Load data from source(s) and apply processing, conversion, and validation procedures.\n\n    See `TidyDataModel.tidy()` for more details.\n\n    Parameters\n    ----------\n    *source : str | pathlib.Path | DataFrame\n        Arbitrary number of reference(s) to data source(s).\n    read_func : Callable\n        Function to load data from source(s).\n    read_options : dict\n        Keyword arguments to pass to `read_func`.\n\n    Returns\n    -------\n    DataFrame\n        Single DataFrame containing data from all source(s) coerced according to class schema.\n    \"\"\"\n    read_func = functools.partial(read_func, schema=cls.schema(), **read_options)\n    data = reader.read(*source, read_func=read_func)\n    process = cls.tidy()\n    return process(data)  # TODO: add option to use TidyDataFrame\n</code></pre>"},{"location":"api/model/model/#tidy_tools.model.TidyDataModel.convert","title":"convert  <code>classmethod</code>","text":"<pre><code>convert(data: DataFrame) -&gt; DataFrame\n</code></pre> <p>Apply conversion functions to supported fields.</p> <p>Outputs messages to logging handlers.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Object to apply conversion functions.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Converted data.</p> Source code in <code>src/tidy_tools/model/model.py</code> <pre><code>@classmethod\ndef convert(cls, data: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Apply conversion functions to supported fields.\n\n    Outputs messages to logging handlers.\n\n    Parameters\n    ----------\n    data : DataFrame\n        Object to apply conversion functions.\n\n    Returns\n    -------\n    DataFrame\n        Converted data.\n    \"\"\"\n    queue = {\n        cls_field: convert_field(\n            cls_field=cls_field, cls_field_exists=cls_field.alias in data.columns\n        )\n        for cls_field in attrs.fields(cls)\n    }\n\n    # return data.withColumns(\n    #     {cls_field.name: column for cls_field, column in queue.items()}\n    # )\n    return data.select(*(column for column in queue.values()))\n</code></pre>"},{"location":"api/model/model/#tidy_tools.model.TidyDataModel.validate","title":"validate  <code>classmethod</code>","text":"<pre><code>validate(data: DataFrame) -&gt; DataFrame\n</code></pre> <p>Apply validation functions to supported fields.</p> <p>Outputs messages to logging handlers.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Object to apply validations functions.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Original data passed to function.</p> Source code in <code>src/tidy_tools/model/model.py</code> <pre><code>@classmethod\ndef validate(cls, data: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Apply validation functions to supported fields.\n\n    Outputs messages to logging handlers.\n\n    Parameters\n    ----------\n    data : DataFrame\n        Object to apply validations functions.\n\n    Returns\n    -------\n    DataFrame\n        Original data passed to function.\n    \"\"\"\n    errors = {\n        cls_field: validate_field(cls_field, data=data)\n        for cls_field in attrs.fields(cls)\n        if cls_field.validator\n    }\n\n    n_rows = data.count()\n    for cls_field, error in errors.items():\n        if error is not None:\n            n_failures = error.data.count()\n            logger.error(\n                f\"Validation(s) failed for `{cls_field.name}`: {n_failures:,} rows ({n_failures / n_rows:.1%})\"\n            )\n        else:\n            logger.success(f\"All validation(s) passed for `{cls_field.name}`\")\n    return data\n</code></pre>"},{"location":"api/model/model/#tidy_tools.model.TidyDataModel.tidy","title":"tidy  <code>classmethod</code>","text":"<pre><code>tidy() -&gt; Callable\n</code></pre> <p>Method for composing processing functions.</p> <p>If present, the methods are executed in the following order:     - pre-processing     - conversions     - validations     - post-processing</p> <p>Returns:</p> Type Description <code>Callable</code> <p>Function to call listed methods.</p> Source code in <code>src/tidy_tools/model/model.py</code> <pre><code>@classmethod\ndef tidy(cls) -&gt; Callable:\n    \"\"\"\n    Method for composing processing functions.\n\n    If present, the methods are executed in the following order:\n        - pre-processing\n        - conversions\n        - validations\n        - post-processing\n\n    Returns\n    -------\n    Callable\n        Function to call listed methods.\n    \"\"\"\n    return compose(\n        cls.__preprocess__, cls.convert, cls.validate, cls.__postprocess__\n    )\n</code></pre>"},{"location":"api/model/validate/","title":"Validate","text":"<p>Apply validation function(s) to schema cls_field.</p> <p>Parameters:</p> Name Type Description Default <code>cls_field</code> <code>Attribute</code> <p>Schema for field in class.</p> required <code>data</code> <code>DataFrame</code> <p>Data to validate field against.</p> required <p>Returns:</p> Type Description <code>TidyError</code> <p>If the validation function fails for at least one row, an error handler is returned for further logging.</p> Source code in <code>src/tidy_tools/model/validate.py</code> <pre><code>def validate_field(cls_field: attrs.Attribute, data: DataFrame) -&gt; TidyError:\n    \"\"\"\n    Apply validation function(s) to schema cls_field.\n\n    Parameters\n    ----------\n    cls_field : attrs.Attribute\n        Schema for field in class.\n    data : DataFrame\n        Data to validate field against.\n\n    Returns\n    -------\n    TidyError\n        If the validation function fails for at least one row, an error handler\n        is returned for further logging.\n    \"\"\"\n    validate_func = _mapper(cls_field.validator)\n    # TODO: add support for TidyDataFrame;\n    # should disable messages to avoid unnecessary filter messages\n    invalid_entries = data.filter(operator.inv(validate_func(cls_field.name)))\n    try:\n        assert invalid_entries.isEmpty()\n        error = None\n    except AssertionError:\n        error = TidyError(cls_field.name, validate_func, invalid_entries)\n    finally:\n        return error\n</code></pre>"},{"location":"api/model/validate/#validators","title":"Validators","text":"<p>Tidy Tools comes with its own custom validators.</p>"},{"location":"api/model/validate/#tidy_tools.model.validators.validate_nulls","title":"validate_nulls","text":"<pre><code>validate_nulls(_defaults: tuple[str] = ('\\\\s*', '\\\\bN/A\\\\b')) -&gt; Callable\n</code></pre> <p>Return expression checking for null values in column.</p> <p>Parameters:</p> Name Type Description Default <code>_defaults</code> <code>tuple[str]</code> <p>Default values representing null. By default, checks for whitespace values and \"N/A\".</p> <code>('\\\\s*', '\\\\bN/A\\\\b')</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Constructs closure that can be called on column(s).</p> Source code in <code>src/tidy_tools/model/validators.py</code> <pre><code>def validate_nulls(_defaults: tuple[str] = (r\"\\s*\", r\"\\bN/A\\b\")) -&gt; Callable:\n    \"\"\"\n    Return expression checking for null values in column.\n\n    Parameters\n    ----------\n    _defaults : tuple[str]\n        Default values representing null. By default, checks for whitespace values and \"N/A\".\n\n    Returns\n    -------\n    Callable\n        Constructs closure that can be called on column(s).\n    \"\"\"\n\n    def closure(column: str) -&gt; Column:\n        return operator.inv(_predicate.is_null(column, _defaults=_defaults))\n\n    return closure\n</code></pre>"},{"location":"api/model/validate/#tidy_tools.model.validators.validate_pattern","title":"validate_pattern","text":"<pre><code>validate_pattern(pattern: str) -&gt; Callable\n</code></pre> <p>Return expression checking for pattern in column.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression to check for in column.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Constructs closure that can be called on column(s).</p> Source code in <code>src/tidy_tools/model/validators.py</code> <pre><code>def validate_pattern(pattern: str) -&gt; Callable:\n    \"\"\"\n    Return expression checking for pattern in column.\n\n    Parameters\n    ----------\n    pattern : str\n        Regular expression to check for in column.\n\n    Returns\n    -------\n    Callable\n        Constructs closure that can be called on column(s).\n    \"\"\"\n\n    def closure(column: str) -&gt; Column:\n        return _predicate.is_regex_match(column, pattern=pattern)\n\n    return closure\n</code></pre>"},{"location":"api/model/validate/#tidy_tools.model.validators.validate_membership","title":"validate_membership","text":"<pre><code>validate_membership(elements: Sequence) -&gt; Callable\n</code></pre> <p>Return expression checking for membership in column.</p> <p>Parameters:</p> Name Type Description Default <code>elements</code> <code>Sequence</code> <p>Collection containing value(s) to check for in column.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Constructs closure that can be called on column(s).</p> Source code in <code>src/tidy_tools/model/validators.py</code> <pre><code>def validate_membership(elements: Sequence) -&gt; Callable:\n    \"\"\"\n    Return expression checking for membership in column.\n\n    Parameters\n    ----------\n    elements : Sequence\n        Collection containing value(s) to check for in column.\n\n    Returns\n    -------\n    Callable\n        Constructs closure that can be called on column(s).\n    \"\"\"\n\n    def closure(column: str) -&gt; Column:\n        return _predicate.is_member(column, elements=elements)\n\n    return closure\n</code></pre>"},{"location":"api/model/validate/#tidy_tools.model.validators.validate_range","title":"validate_range","text":"<pre><code>validate_range(lower_bound: Any, upper_bound: Any) -&gt; Callable\n</code></pre> <p>Return expression checking for inclusion in column.</p> <p>Parameters:</p> Name Type Description Default <code>lower_bound</code> <code>Any</code> <p>Least value to check for in column.</p> required <code>upper_bound</code> <code>Any</code> <p>Greatest value to check for in column.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>Constructs closure that can be called on column(s).</p> Source code in <code>src/tidy_tools/model/validators.py</code> <pre><code>def validate_range(lower_bound: Any, upper_bound: Any) -&gt; Callable:\n    \"\"\"\n    Return expression checking for inclusion in column.\n\n    Parameters\n    ----------\n    lower_bound : Any\n        Least value to check for in column.\n    upper_bound : Any\n        Greatest value to check for in column.\n\n    Returns\n    -------\n    Callable\n        Constructs closure that can be called on column(s).\n    \"\"\"\n\n    def closure(column: str) -&gt; Column:\n        return _predicate.is_between(column, boundaries=(lower_bound, upper_bound))\n\n    return closure\n</code></pre>"},{"location":"api/workflow/compose/","title":"Compose","text":"<p>Define and store pipeline as object to be executed.</p> <p>Unlike <code>pipe</code>, <code>compose</code> will not evaluate when initialized. These are two separate steps. See Examples for more details.</p> <p>Parameters:</p> Name Type Description Default <code>*functions</code> <code>Callable</code> <p>Arbitrary number of functions to chain together.</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable</code> <p>Nested function in order of function(s) passed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # works with unary function\n&gt;&gt;&gt; add_two = lambda x: x + 2\n&gt;&gt;&gt;\n&gt;&gt;&gt; # works with partial functions\n&gt;&gt;&gt; add_n = lambda x, n: x + n\n&gt;&gt;&gt;\n&gt;&gt;&gt; # works with closures\n&gt;&gt;&gt; def add_n(n: int) -&gt; Callable:\n&gt;&gt;&gt;     def closure(x):\n&gt;&gt;&gt;         return x + n\n&gt;&gt;&gt;     return closure\n&gt;&gt;&gt;\n&gt;&gt;&gt; summation = compose(add_two, add_n(10), add_n(-4))\n&gt;&gt;&gt; assert summation(12) == 20\n</code></pre> Source code in <code>src/tidy_tools/workflow/pipeline.py</code> <pre><code>def compose(*functions: Callable) -&gt; Callable:\n    \"\"\"\n    Define and store pipeline as object to be executed.\n\n    Unlike `pipe`, `compose` will not evaluate when initialized.\n    These are two separate steps. See Examples for more details.\n\n    Parameters\n    ----------\n    *functions : Callable\n        Arbitrary number of functions to chain together.\n\n    Returns\n    -------\n    Callable\n        Nested function in order of function(s) passed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # works with unary function\n    &gt;&gt;&gt; add_two = lambda x: x + 2\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # works with partial functions\n    &gt;&gt;&gt; add_n = lambda x, n: x + n\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # works with closures\n    &gt;&gt;&gt; def add_n(n: int) -&gt; Callable:\n    &gt;&gt;&gt;     def closure(x):\n    &gt;&gt;&gt;         return x + n\n    &gt;&gt;&gt;     return closure\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; summation = compose(add_two, add_n(10), add_n(-4))\n    &gt;&gt;&gt; assert summation(12) == 20\n    \"\"\"\n    return lambda instance: pipe(instance, *functions)\n</code></pre>"},{"location":"api/workflow/pipeline/","title":"Pipeline","text":"<p>Apply arbitrary number of functions to <code>instance</code> in succession.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>Any</code> <p>Scalar object.</p> required <code>*functions</code> <code>Callable</code> <p>Functions.</p> <code>()</code> <p>Returns:</p> Type Description <code>Any</code> <p>Result of applying all function(s) to <code>instance</code>. Does not necessarily need to be the same type as it was at the start of the pipeline.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # works with unary function\n&gt;&gt;&gt; add_two = lambda x: x + 2\n&gt;&gt;&gt;\n&gt;&gt;&gt; # works with partial functions\n&gt;&gt;&gt; add_n = lambda x, n: x + n\n&gt;&gt;&gt;\n&gt;&gt;&gt; # works with closures\n&gt;&gt;&gt; def add_n(n: int) -&gt; Callable:\n&gt;&gt;&gt;     def closure(x):\n&gt;&gt;&gt;         return x + n\n&gt;&gt;&gt;     return closure\n&gt;&gt;&gt;\n&gt;&gt;&gt; result = pipe(12, add_two, add_n(10), add_n(-4))\n&gt;&gt;&gt; assert result == 20\n</code></pre> Source code in <code>src/tidy_tools/workflow/pipeline.py</code> <pre><code>def pipe(instance: Any, *functions: Callable) -&gt; Any:\n    \"\"\"\n    Apply arbitrary number of functions to `instance` in succession.\n\n    Parameters\n    ----------\n    instance : Any\n        Scalar object.\n    *functions : Callable\n        Functions.\n\n    Returns\n    -------\n    Any\n        Result of applying all function(s) to `instance`. Does not necessarily\n        need to be the same type as it was at the start of the pipeline.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # works with unary function\n    &gt;&gt;&gt; add_two = lambda x: x + 2\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # works with partial functions\n    &gt;&gt;&gt; add_n = lambda x, n: x + n\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # works with closures\n    &gt;&gt;&gt; def add_n(n: int) -&gt; Callable:\n    &gt;&gt;&gt;     def closure(x):\n    &gt;&gt;&gt;         return x + n\n    &gt;&gt;&gt;     return closure\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; result = pipe(12, add_two, add_n(10), add_n(-4))\n    &gt;&gt;&gt; assert result == 20\n    \"\"\"\n    return functools.reduce(lambda init, func: func(init), functions, instance)\n</code></pre>"},{"location":"development/roadmap/","title":"Roadmap","text":"<p>There are a couple directions I would like to take Tidy Tools given the time. Although I'm happy making these changes even if it just benefits myself, any input from the community would be greatly appreciated! I will try my best to update this roadmap as the project continues to evolve.</p>"},{"location":"development/roadmap/#observability-and-logging-support","title":"Observability and Logging Support","text":"<p>As of writing, <code>TidyDataFrame</code> and <code>TidyDataModel</code> both include logging functionality. Although these features are helpful, they do not yet feel complete. To feel complete, Tidy Tools would need:</p> <ul> <li>an extensive <code>exceptions</code> and <code>logging</code> layer</li> <li>support across the whole package</li> <li>(ideally) parsing for logging analytics</li> </ul> <p>Thankfully, <code>loguru</code> simplifies a lot of the logging headaches that I would have to address otherwise. I'll just need to find the time to incorporate these ideas.</p>"},{"location":"development/roadmap/#orchestration-layer","title":"Orchestration Layer","text":"<p>Building on the observability goal, I'd like to offer a module that can sit on top of Tidy Tools to facilitate the execution of tidy workflows. I think <code>TidyDataModel</code> includes some preliminary thoughts on this, but there is more that can be included.</p>"},{"location":"development/roadmap/#support-for-other-dataframe-libraries","title":"Support for Other DataFrame Libraries","text":"<p>PySpark was an easy first target:</p> <ul> <li>It's a popular language.</li> <li>It's the language of choice at my workplace.</li> <li>It's built on Java so it won't always follow pythonic conventions.</li> </ul> <p>However, I would like to extend this to other libraries. There are other packages like <code>pandera</code> and <code>patito</code> out there already, and I don't want to rewrite the work these projects have already done. If not for wide-adoption, it'd at least be a great learning experience.</p>"},{"location":"development/roadmap/#the-inevitable-re-write","title":"The Inevitable Re-write","text":"<p>I've tried my best to develop code that will last. However, requirements and preferences change over time, and Tidy Tools might not look the same now as it will a couple dozen commits from now. Although I will try to delay this as much as possible (especially if I'm the only one using this), my main reasons as of writing are:</p> <ul> <li>Inexperience: I'd describe myself as a strong intermediate Pythonista and nothing more. Aside from taking CS 105 and the many semesters teaching the class, I'm going off what I've taught myself.</li> <li>Rust: This would be a perfect excuse for picking up the language.</li> <li>Community Support: Although I've written and thought up most of this project, that doesn't need to always be the case. I'd be happy to learn from others and integrate those changes to make Tidy Tools an even more impactful project.</li> </ul>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>Tidy Tools can be installed using a simple one-line command.</p> <pre><code>pip install tidy-tools\n</code></pre>"},{"location":"user-guide/installation/#importing","title":"Importing","text":"<p>To use the package, import it into your project:</p> <pre><code># import top-level package (includes welcome message)\nimport tidy_tools\n\n# import specific modules as needed\nfrom tidy_tools.core import selectors as cs, filter as ttf\nfrom tidy_tools.frame import TidyDataFrame\nfrom tidy_tools.models import TidyDataModel\nfrom tidy_tools.workflow import pipe, compose\n</code></pre>"},{"location":"user-guide/modules/core/","title":"Core","text":"<p>The <code>tidy_tools.core</code> module is a functional replacement of the \"basic\" queries, namely:</p> <ul> <li>Selecting columns</li> <li>Filtering data</li> </ul>"},{"location":"user-guide/modules/core/#motivation","title":"Motivation","text":"<p>The core module stemmed from a couple ideas, but the most pressing examples are:</p> <ul> <li>The tedious nature of applying a filter across multiple columns</li> <li>The intuitive column selectors module in Polars</li> </ul> <p>For this section, I'll only focus on the first example. Consider the following PySpark code that attempts to filter null values in a DataFrame.</p> <pre><code>from pyspark.sql import functions as F\n\nresult = spark_data.filter(\n    (F.col('A').isNull() | F.col('A').rlike(r\"^\\s*$\")) |\n    (F.col('B').isNull() | F.col('B').rlike(r\"^\\s*$\")) |\n    (F.col('C').isNull() | F.col('C').rlike(r\"^\\s*$\"))\n)\n</code></pre> <p>Let's read through this query. There are three main aspects:</p> <ul> <li>our predicate checks for values that are <code>NULL</code> or entirely whitespace</li> <li>our predicate is applied to multiple columns</li> <li>our predicates are reduced into a single expression</li> </ul> <p>The basis of all PySpark queries can be expressed as such: we want to apply a predicate to at least one column that evaluates to a PySpark expression. However, we are not limited to the PySpark API.</p> <p>Let's revisit this example using a functional approach:</p> <pre><code>import functools\nimport operator\n\nfrom pyspark.sql import (\n    Column,\n    DataFrame,\n    functions as F\n)\n\ndef is_null(column: str | Column) -&gt; Column:\n    \"\"\"Return expression identifying null values.\"\"\"\n    if not isinstance(column, Column):\n        column = F.col(column)\n    return column.isNull() | column.rlike(r\"^\\s*$\")\n\ndef filter_null(\n    data: DataFrame,\n    *columns: str | Column,\n    strict: bool = False,\n    invert: bool = False\n) -&gt; DataFrame:\n    \"\"\"Filter data for null values.\"\"\"\n    # define, apply predicate to multiple columns (by default, all)\n    predicate = map(is_null, columns or data.columns)\n    # reduce predicates into single expression\n    comparison_op = operator.and_ if strict else operator.or_\n    query = functools.reduce(comparison_op, predicate)\n    # evaluate expression on data\n    return data.filter(operator.inv(query) if invert else query)\n</code></pre> <p>This seems like even more work for filtering null values than before. Luckily only I will be writing this. Let's see what you'll be writing instead and compare it to how you would write it in native PySpark.</p> <pre><code># filtering one column for null values\npyspark_result = spark_data.filter(F.col('A').isNull() | F.col('A').rlike(r\"^\\s*$\"))\ntidy_result = filter_null(spark_data, 'A')\n\n# filtering on multiple columns for *any* null values\npyspark_result = spark_data.filter(\n    (F.col('A').isNull() | F.col('A').rlike(r\"^\\s*$\")) |\n    (F.col('B').isNull() | F.col('B').rlike(r\"^\\s*$\")) |\n    (F.col('C').isNull() | F.col('C').rlike(r\"^\\s*$\"))\n)\ntidy_result = filter_null(spark_data, 'A', 'B', 'C')\n\n# filtering on multiple columns for *all* null values\npyspark_result = spark_data.filter(\n    (F.col('A').isNull() | F.col('A').rlike(r\"^\\s*$\")) &amp;\n    (F.col('B').isNull() | F.col('B').rlike(r\"^\\s*$\")) &amp;\n    (F.col('C').isNull() | F.col('C').rlike(r\"^\\s*$\"))\n)\ntidy_result = filter_null(spark_data, 'A', 'B', 'C', strict=True)\n\n# filtering on multiple columns for *no* null values\npyspark_result = spark_data.filter(\n    ~ (\n        (F.col('A').isNull() | F.col('A').rlike(r\"^\\s*$\")) |\n        (F.col('B').isNull() | F.col('B').rlike(r\"^\\s*$\")) |\n        (F.col('C').isNull() | F.col('C').rlike(r\"^\\s*$\"))\n    )\n)\ntidy_result = filter_null(spark_data, 'A', 'B', 'C', invert=True)\n</code></pre> <p>All filtering expressions reduce these tedious elements of PySpark expressions to intuitive, easy-to-control parameters. This is just one example of how Tidy Tools promotes declarative workflows, letting you focus on the 'what to do' and not the 'how to do'.</p>"},{"location":"user-guide/modules/dataframe/","title":"DataFrame","text":"<p>The <code>tidy_tools.dataframe</code> module offers the most direct extension of the PySpark DataFrame by offering a wrapper called <code>TidyDataFrame</code> - the initial motivation for Tidy Tools.</p> <p>This module attempts to achieve the following goals:</p> <ul> <li>Promoting user-friendly features:<ul> <li>Built-in logging (inspired by <code>tidylog</code> in R)</li> <li>Contextual evaluation</li> </ul> </li> <li>Incorporting the rest of the Tidy Tools ecosystem</li> </ul>"},{"location":"user-guide/modules/dataframe/#tidydataframe","title":"TidyDataFrame","text":"<p>Starting out as a basic Python class, TidyDataFrame has evolved from a dataframe with built-in logging to providing functional recipes to handling user-specific configurations and more. All methods provided in Tidy Tools are supported by <code>TidyDataFrame</code> as it is designed with this class in mind.</p> <p>Let's look at an example of how to use <code>TidyDataFrame</code> compared to a PySpark DataFrame.</p>"},{"location":"user-guide/modules/dataframe/#entering-and-exiting-tidydataframe","title":"Entering and Exiting TidyDataFrame","text":"<p>Creating a <code>TidyDataFrame</code> is as simple as wrapping your existing PySpark DataFrame with the <code>TidyDataFrame()</code> function. Once inside a TidyDataFrame, all supported methods will output a one-line message to the console detailing the exact impact it has on your data.</p> <pre><code>from tidy_tools.dataframe import TidyDataFrame\n\n\nspark_data = ...\ntidy_data = TidyDataFrame(spark_data) # initialize a TidyDataFrame instance\n</code></pre> <p>Once you no longer require the logging services of a <code>TidyDataFrame</code>, you may exit the context by calling <code>TidyDataFrame.data</code>. This method will return the underlying PySpark DataFrame stored inside the tidy context.</p> <pre><code>underlying_data = tidy_data.data\n</code></pre> <p>Altogether, a tidy workflow will look like this:</p> <pre><code># create PySpark DataFrame\nspark_data = ...\n\n# convert to a TidyDataFrame\ntidy_data = TidyDataFrame(spark_data)\n\n# perform native operations - messages returned to console\ntidy_data = (\n    tidy_data\n    .select(...)\n    .filter(...)\n    .withColumn(...)\n)\n\n# convert back to a PySpark DataFrame\nspark_data = tidy_data.data\n</code></pre>"},{"location":"user-guide/modules/dataframe/#extending-the-tidydataframe","title":"Extending the TidyDataFrame","text":"<p>Discuss <code>TidyContext</code>...</p>"},{"location":"user-guide/modules/models/","title":"Models","text":"<p>Tidy Tools prioritizes development around the <code>TidyDataFrame</code> module. However, not all workflows require such a hands-on approach, where all conversions and validations are applied directly on/through the data. Instead, what if you could reduce these operations to their barest form, abstracting the PySpark DataFrame API out of your business logic?</p> <p>The <code>TidyDataModel</code> is a class-level implementation of <code>TidyDataFrame</code>. Rather than writing queries one-by-one and having to track things like schema evolution, (implicit) field validations, and dataframe-level relationships, operations can be defined all in one place. This greatly simplifies the development and review process since all things can be tracked back to the <code>TidyDataModel</code> instance.</p>"},{"location":"user-guide/modules/models/#tidydataframe-vs-tidydatamodel","title":"TidyDataFrame vs TidyDataModel","text":""},{"location":"user-guide/modules/models/#design","title":"Design","text":"<p>As discussed on the previous page, <code>TidyDataFrame</code> is a wrapper around the native PySpark DataFrame class. Although <code>TidyDataModel</code> is also a Python class, it is built on attrs, a robust class building package.</p>"},{"location":"user-guide/modules/models/#usage","title":"Usage","text":"<p>All data workflows incorporate the following principals to various degrees:</p> <ul> <li> <p>Conversions: manipulating data into different shapes</p> </li> <li> <p>Validations: asserting expectations of your data</p> </li> </ul> <p>As we will see below, <code>TidyDataFrame</code> - just like the Pyspark DataFrame - greatly simplifies the conversion workflow by incorporating logging messages. However, it falls short in the validation aspect, an area that <code>TidyDataModel</code> thrives in. Let's observe both below.</p>"},{"location":"user-guide/modules/models/#conversions","title":"Conversions","text":"<p>Let's use the following example based on the California Housing market dataset. We must perform following conversions:</p> <ul> <li> <p>Convert <code>latitude</code> and <code>longitude</code> to float types.</p> </li> <li> <p>Scale <code>income</code> by the CAD foreign exchange rate.</p> </li> </ul> <pre><code>import decimal\n\nfrom attrs import define, field\n\nfrom pyspark.sql import types as T, functions as F\n\nfrom tidy_tools.dataframe import TidyDataFrame\nfrom tidy_tools.model import TidyDataModel\n\n\ndef convert_fx(currency: str) -&gt; Callable:\n    \"\"\"Closure for returning a function that contains the appropriate currency conversion\"\"\"\n\n    match currency.strip().upper():\n        case \"CAD\":\n            match_rate = 0.7\n        case \"YEN\":\n            match_rate = 70\n        case _:\n            match_rate = 1\n\n    def convert(column: Column) -&gt; Column:\n        \"\"\"Scale a currency column by the specified rate\"\"\"\n        return column * match_rate\n\n    return convert\n\n\n# load data\nspark_data = spark.read.csv(\"california_housing.csv\")\n\n# apply conversions with TidyDataFrame\ntidy_data = (\n    TidyDataFrame(spark_data)\n    .withColumns({\n        column: F.col(column).cast(T.FloatType())\n        for column in (longitude, latitude)\n    })\n    .withColumn(\"median_income\", convert_fx(currency=\"CAD\")(\"median_income\"))\n)\n\n# apply conversions with TidyDataModel\n@define\nclass CaliforniaHousing(TidyDataModel):\n    longitude: float\n    latitude: float\n    median_age: int\n    rooms: int\n    bedrooms: int\n    population: int\n    households: int\n    median_income: decimal.Decimal = field(converter=convert_fx(currency=\"CAD\"))\n    value: decimal.Decimal\n\ntidy_data = CaliforniaHousing.read(\"california_housing.csv\")\n</code></pre> <p>Using <code>TidyDataFrame</code>, we can easily address all requirements using PySpark's DataFrame API (with the added bonus of line-by-line logging messages). However, notice that <code>TidyDataModel</code> can also perform the task in a syntax that does not explicitly rely on the PySpark DataFrame API. Simply specify what needs to be converted and nothing more.</p> <p>Addressing the elephant in the room, <code>TidyDataModel</code> will always require more setup than <code>TidyDataFrame</code>. This is because a data model should represent a complete and accurate model of your data, something <code>TidyDataFrame</code> cannot and should not incorporate by default.</p>"},{"location":"user-guide/modules/models/#validations","title":"Validations","text":"<p>Our client is happy with the conversions, but they want to be sure that the data meets their strict requirements. Let's try to validate the following:</p> <ul> <li> <p><code>latitude</code> is between (-90, 90) degrees</p> </li> <li> <p><code>longitude</code> is between (-180, 180) degrees</p> </li> </ul> <pre><code># apply conversions, validations with TidyDataFrame\ntidy_data = (\n    TidyDataFrame(spark_data)\n    .withColumns({\n        column: F.col(column).cast(T.FloatType())\n        for column in (longitude, latitude)\n    })\n    .withColumn(\"median_income\", convert_fx(currency=\"CAD\")(\"median_income\"))\n)\nassert tidy_data.filter(~F.col(\"latitude\").between(-90, 90)).isEmpty()\nassert tidy_data.filter(~F.col(\"longitude\").between(-180, 180)).isEmpty()\n\n# apply conversions, validations with TidyDataModel\ndef validate_range(column: Column, lower: int, upper: int) -&gt; Column:\n    return column.between(lower, upper)\n\n@define\nclass CaliforniaHousing(TidyDataModel):\n    longitude: float = field(validator=validate_range(-90, 90))\n    latitude: float = field(validator=validate_range(-180, 180))\n    median_age: int\n    rooms: int\n    bedrooms: int\n    population: int\n    households: int\n    median_income: decimal.Decimal = field(converter=convert_fx(currency=\"CAD\"))\n    value: decimal.Decimal\n\ntidy_data = CaliforniaHousing.read(\"california_housing.csv\")\n</code></pre> <p>Notice how the <code>TidyDataFrame</code> workflow now has two rogue <code>assert</code> statements at the end of the workflow. As of right now, should either condition fail, the code comes to a halt and the user must debug the error themselves. This task can be extremely tedious and lost in the midst of all your conversion operations.</p> <p>In contrast, <code>TidyDataModel</code> encapsulates the validation logic in the same location as the conversion logic. We already had a clear picture of our data since the model details all the fields we expect. Now we have an even clearer picture of what is and is not true of our data for the validations we specified.</p> <p>Additionally, notice that this method also required little to no knowledge of the PySpark DataFrame API or Python's <code>assert</code> statement, building on the goal of separating language-specific features from your business logic.</p>"},{"location":"user-guide/modules/models/#validators","title":"Validators","text":"<p>Similar to the <code>attrs</code> package, <code>tidy-tools</code> comes with its own set of validators. These functions evaluate to filtering expressions that attempt to assert some condition is true. Specifically, given a condition, we expect zero rows to not meet said condition.</p> <p>A list of built-in validators include:</p> <ul> <li> <p><code>validate_nulls</code></p> </li> <li> <p><code>validate_regex</code></p> </li> <li> <p><code>validate_membership</code></p> </li> <li> <p><code>validate_range</code></p> </li> </ul> <p>The priority now is to develop native validators that would be helpful for users. However, the end goal is to only provide validators that cannot be constructed using the <code>attrs.validators</code> module.</p>"},{"location":"user-guide/modules/workflow/","title":"Workflow","text":"<p>As of PySpark <code>v3.5</code>, the only support for piping methods is invoking <code>DataFrame.transform</code>, like such:</p> <pre><code>from pyspark.sql import DataFrame\n\n\ndef f(data: DataFrame) -&gt; DataFrame:\n    ...\n\ndef g(data: DataFrame) -&gt; DataFrame:\n    ...\n\ndef h(data: DataFrame) -&gt; DataFrame:\n    ...\n\n\ntransformed_data = data.transform(f).transform(g).transform(h)\n</code></pre> <p>Although this approach works, it's limiting since only one custom transformation function can be applied per <code>DataFrame.transform</code>. Additionally, there is no insight into the procedures, making this difficult to trace should an error arise. Ideally, users should be able to invoke multiple transformations in one function call and store procedures to be called at a later time with insight into the chain of commands.</p> <p>Tidy Tools addresses this by providing the <code>workflow</code> module. Users can expect to compose simple pipelines that provide visibility into procedures.</p> <pre><code>from pyspark.sql import DataFrame\nfrom tidy_tools.workflow import pipe, compose\n\n\ndef f(data: DataFrame) -&gt; DataFrame:\n    ...\n\ndef g(data: DataFrame) -&gt; DataFrame:\n    ...\n\ndef h(data: DataFrame) -&gt; DataFrame:\n    ...\n\n\n# execute transformation functions in succession\ntransformed_data = pipe(data, f, g, h)\n\n# or, store transformation functions as single callable function\npipeline = compose(f, g, h)\ntransformed_data = pipeline(data)\n</code></pre> <p>Both approaches reduce the manual process of chaining <code>DataFrame.transform</code> multiple times. Additionally, <code>compose()</code> lets you store common procedures, giving users a simple method for chaining transformation functions into a single function.</p>"}]}